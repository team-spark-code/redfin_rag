# # -*- coding: utf-8 -*- 2025-09-04 10:05
# """
# news_service.py
# - Fix: body_mdì— LLM JSON ì „ì²´ê°€ ë¬¸ìì—´ë¡œ ì €ì¥ë˜ëŠ” ë¬¸ì œ í•´ê²°
# - Change: LLM ì¶œë ¥ íŒŒì‹± ê°•í™”(íœìŠ¤ëœ ```json ë¸”ë¡ ìš°ì„ ), ì‹¤íŒ¨ ì‹œì—ë„ body_mdë§Œ ì¶”ì¶œ
# - Change: publish_from_source ê°€ NewsPost ìŠ¤í‚¤ë§ˆë¡œ ì¼ê´€ ì €ì¥(tldr í¬í•¨)
# - Cleanup: ì¤‘ë³µ/ë¶ˆí•„ìš” import ì œê±°, ê°€ë“œ ê°„ì†Œí™”
# (2025-09-04)
# """
# from __future__ import annotations

# import json
# import os
# import re
# from datetime import datetime
# from pathlib import Path
# from typing import Any, Dict, List, Optional
# from uuid import uuid4

# from pymongo import MongoClient, UpdateOne

# from core.settings import settings
# from nureongi.loaders import NewsLoader, DEFAULT_FIELD_MAP
# from nureongi.prompt_loader import load_md_template, render_template
# from nureongi.vectorstore import create_chroma_store
# from nureongi.vs_news import upsert_news_post_to_chroma
# from observability.mongo_logger import get_news_collection
# from schemas.news import NewsPublishRequest, NewsPost
# from schemas.news_llm import NewsLLMOut

# # LLM/ì²´ì¸
# from nureongi.llm import build_default_llm
# from nureongi.news_chain import build_news_chain

# # Optional chunkers
# try:
#     from langchain_huggingface import HuggingFaceEmbeddings
# except Exception:
#     HuggingFaceEmbeddings = None  # type: ignore

# # Text splitters(ë²„ì „ ëŒ€ì‘)
# try:
#     from langchain_text_splitters import RecursiveCharacterTextSplitter
# except Exception:  # pragma: no cover
#     from langchain.text_splitter import RecursiveCharacterTextSplitter  # type: ignore


# # --------------------------------------
# # ì‘ì€ ìœ í‹¸
# # --------------------------------------

# def _as_bool(v: object | None, default: bool = False) -> bool:
#     if isinstance(v, bool):
#         return v
#     if v is None:
#         return default
#     return str(v).strip().lower() in {"1", "true", "yes", "on"}


# def _as_int(v: object | None, default: int = 0) -> int:
#     try:
#         return int(str(v).strip()) if v is not None and str(v).strip() != "" else default
#     except Exception:
#         return default


# def _coerce_text(x: Any) -> str:
#     """LLM/ì²´ì¸ ë°˜í™˜ê°’ì„ ë¬¸ìì—´ë¡œ ì •ê·œí™”."""
#     try:
#         if hasattr(x, "content"):
#             return str(getattr(x, "content"))
#         if isinstance(x, (dict, list)):
#             return json.dumps(x, ensure_ascii=False)
#         return str(x)
#     except Exception:
#         return str(x)


# _JSON_FENCE_RE = re.compile(r"```json\s*(\{[\s\S]*?\})\s*```", re.IGNORECASE)


# def _safe_json_block(s: str) -> Dict[str, Any]:
#     """ë¬¸ìì—´ì—ì„œ JSON ë¸”ë¡ë§Œ ë½‘ì•„ íŒŒì‹±.
#     1) ```json ... ``` íœìŠ¤ ìš°ì„ 
#     2) ì „ì²´ ë¬¸ìì—´ì—ì„œ ê°€ì¥ ë°”ê¹¥ { ... } ë²”ìœ„ ì¶”ì •
#     ì‹¤íŒ¨ ì‹œ ValueError ë°œìƒ
#     """
#     s = (s or "").strip()
#     m = _JSON_FENCE_RE.search(s)
#     if m:
#         return json.loads(m.group(1))
#     i, j = s.find("{"), s.rfind("}")
#     if i != -1 and j != -1 and j > i:
#         return json.loads(s[i : j + 1])
#     # ëª…ë°±í•œ JSON í˜•ì‹ì´ ì•„ë‹ ë•ŒëŠ” ì‹¤íŒ¨
#     raise ValueError("no json block")


# def _parse_llm_output_any(x: Any) -> Optional[NewsLLMOut]:
#     """LLM ì¶œë ¥(any)ì„ NewsLLMOutë¡œ íŒŒì‹±. ì‹¤íŒ¨ ì‹œ None."""
#     try:
#         data = x if isinstance(x, dict) else _safe_json_block(_coerce_text(x))
#     except Exception:
#         return None
#     try:
#         # pydantic v2 ìš°ì„ 
#         if hasattr(NewsLLMOut, "model_validate"):
#             return NewsLLMOut.model_validate(data)  # type: ignore[attr-defined]
#         return NewsLLMOut.parse_obj(data)  # type: ignore[attr-defined]
#     except Exception:
#         return None


# def _listify(v: Any) -> List[str]:
#     if v is None:
#         return []
#     if isinstance(v, list):
#         return [str(x) for x in v if x is not None]
#     return [str(v)]


# def _dedupe_key(item: Dict[str, Any]) -> Dict[str, Any]:
#     q: Dict[str, Any] = {}
#     if item.get("article_code"):
#         q["article_code"] = str(item["article_code"])  # ìš°ì„ ìˆœìœ„ ë†’ìŒ
#     elif item.get("article_id"):
#         q["article_id"] = str(item["article_id"])
#     elif item.get("url"):
#         q["url"] = str(item["url"])
#     return q


# def _exists_in_news(q: Dict[str, Any]) -> bool:
#     if not q:
#         return False
#     col = get_news_collection()
#     return bool(col and col.find_one(q))


# def _fallback_tldr_from_text(text: str, k: int = 3) -> List[str]:
#     sents = re.split(r"(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+", (text or "").strip())
#     out: List[str] = []
#     for s in sents:
#         s = s.strip()
#         if not s:
#             continue
#         out.append(s[:120])
#         if len(out) >= max(1, k):
#             break
#     return out


# # --------------------------------------
# # ì¸ë±ìŠ¤ ì´ˆê¸°í™”(ìš”ì•½)
# # --------------------------------------

# def init_index() -> dict:
#     try:
#         vs = create_chroma_store(
#             collection_name=settings.news.collection,
#             persist_dir=settings.news.persist_dir,
#             distance="cosine",
#         )
#         vs.persist()
#         return {"ok": True, "collection": settings.news.collection, "persist_dir": settings.news.persist_dir}
#     except Exception as e:
#         return {"ok": False, "error": str(e)}


# def init_news_index_fixed(*, chunk_size: int = 1200, chunk_overlap: int = 120) -> dict:
#     try:
#         if not settings.news.api_url:
#             return {"ok": False, "reason": "settings.news.api_url is empty"}
#         loader = NewsLoader(field_map=DEFAULT_FIELD_MAP)
#         docs = loader.load_news_json(settings.news.api_url, timeout=15)
#         if not docs:
#             return {"ok": False, "reason": "no documents loaded", "api_url": settings.news.api_url}
#         splitter = RecursiveCharacterTextSplitter(
#             chunk_size=chunk_size,
#             chunk_overlap=chunk_overlap,
#             separators=["\n\n", "\n", ". ", "? ", "! ", ", ", " ", ""],
#         )
#         chunks = splitter.split_documents(docs)
#         if HuggingFaceEmbeddings is None:
#             return {"ok": False, "reason": "huggingface embeddings unavailable"}
#         embedding = HuggingFaceEmbeddings(model_name=settings.news.emb_model)
#         vs = create_chroma_store(
#             embedding=embedding,
#             collection_name=settings.news.collection,
#             persist_dir=settings.news.persist_dir,
#             distance="cosine",
#         )
#         vs.vs.add_documents(chunks)
#         vs.persist()
#         return {
#             "ok": True,
#             "mode": "fixed",
#             "api_url": settings.news.api_url,
#             "collection": settings.news.collection,
#             "persist_dir": settings.news.persist_dir,
#             "docs": len(docs),
#             "chunks": len(chunks),
#             "chunk_size": chunk_size,
#             "chunk_overlap": chunk_overlap,
#         }
#     except Exception as e:
#         return {"ok": False, "error": str(e)}


# # --------------------------------------
# # ë‹¨ê±´ ìƒì„±(í•µì‹¬)
# # --------------------------------------

# def generate_news_post(
#     req: NewsPublishRequest,
#     feed_meta: Optional[Dict[str, Any]] = None,
#     *,
#     run_config: Optional[Dict[str, Any]] = None,
# ) -> NewsPost:
#     """LLMì„ ì‚¬ìš©í•´ ë‰´ìŠ¤ í¬ìŠ¤íŠ¸ë¥¼ ìƒì„±(í˜¹ì€ íŒ¨ìŠ¤ìŠ¤ë£¨)í•˜ê³ , NewsPost ìŠ¤í‚¤ë§ˆë¡œ ì €ì¥.
#     - body_mdì— JSON ì „ì²´ê°€ ë“¤ì–´ê°€ì§€ ì•Šë„ë¡ ê°•ì œ ì •ê·œí™”
#     - tldrëŠ” ë³„ë„ ë¦¬ìŠ¤íŠ¸ í•„ë“œë¡œ ì €ì¥
#     """
#     # 1) í…œí”Œë¦¿ ë¡œë”©/ë Œë”ë§(ì²´ì¸ ë‚´ë¶€ ì‚¬ìš©ìš©)
#     tpl_path = settings.news.prompt_path or "src/prompts/templates/news_publish_v1.md"
#     tpl = load_md_template(tpl_path)

#     # 2) LLM ì‹¤í–‰(or ë¯¸ì‚¬ìš©)
#     llm_out: Optional[NewsLLMOut] = None
#     raw_text: str = ""
#     if settings.news.use_llm:
#         llm = build_default_llm()
#         chain = build_news_chain(llm, tpl_path)
#         inputs = {
#             "title": (req.title or ""),
#             "url": (req.url or ""),
#             "content": (req.content or ""),
#             "categories": ", ".join(req.categories or []),
#             "tags": ", ".join(req.tags or []),
#             "meta": json.dumps(feed_meta or {}, ensure_ascii=False),
#         }
#         try:
#             raw_text = _coerce_text(chain.invoke(inputs, config=(run_config or {})))
#             llm_out = _parse_llm_output_any(raw_text)
#         except Exception:
#             llm_out = None
#             raw_text = _coerce_text(raw_text)
#     else:
#         # LLM ë¯¸ì‚¬ìš©: ê·¸ëŒ€ë¡œ íŒ¨ìŠ¤ìŠ¤ë£¨
#         raw_text = req.content or ""

#     # 3) ê²°ê³¼ ì •ê·œí™”: ë°˜ë“œì‹œ body_md(ë§ˆí¬ë‹¤ìš´), tldr(ë¦¬ìŠ¤íŠ¸)ë§Œ ì €ì¥
#     if llm_out:
#         title = llm_out.title or (req.title or "Untitled")
#         body_md = llm_out.body_md or (req.content or "")
#         tldr_list = llm_out.tldr or _fallback_tldr_from_text(body_md, k=3)
#         tags = llm_out.tags or (req.tags or [])
#         sources = llm_out.sources or []
#         hero = llm_out.hero_image_url or None
#         subtitle = llm_out.subtitle or None
#         author = llm_out.author_name or getattr(req, "author_name", None)
#     else:
#         # JSON ê°™ì•„ ë³´ì´ë©´ body_mdë¡œ ì“°ì§€ ì•ŠìŒ(íŒŒì‹± ì‹¤íŒ¨í•œ JSON ì „ì²´ê°€ ì €ì¥ë˜ëŠ” ê±¸ ë°©ì§€)
#         looks_like_json = raw_text.strip().startswith("{") and raw_text.strip().endswith("}")
#         if looks_like_json:
#             try:
#                 data = _safe_json_block(raw_text)
#                 body_md = str(data.get("body_md", "")) or (req.content or "")
#                 tldr_list = _listify(data.get("tldr")) or _fallback_tldr_from_text(body_md, 3)
#             except Exception:
#                 body_md = req.content or ""
#                 tldr_list = _fallback_tldr_from_text(body_md, 3)
#         else:
#             body_md = raw_text or (req.content or "")
#             tldr_list = _fallback_tldr_from_text(body_md, 3)
#         title = req.title or "Untitled"
#         tags, sources, hero, subtitle = (req.tags or []), [], None, None
#         author = getattr(req, "author_name", None)

#     category1 = (req.categories[0] if req.categories else None)

#     post = NewsPost(
#         post_id=str(uuid4()),
#         article_id=req.article_id,
#         article_code=getattr(req, "article_code", None),
#         url=req.url,
#         title=title,
#         dek=subtitle,
#         tldr=tldr_list,
#         body_md=body_md,
#         hero_image_url=hero,
#         author_name=author,
#         category=category1,
#         categories=req.categories or [],
#         tags=tags,
#         sources=sources,
#         status="published" if req.publish else "draft",
#         model_meta={
#             "persona": "news_brief",
#             "strategy": "auto",
#             "top_k": req.top_k,
#             "feed": feed_meta or {},
#             "parsed_ok": bool(llm_out is not None),
#             "service": getattr(settings.news, "service_name", "redfin_news"),
#             "ls_project": getattr(settings.news, "langsmith_project", None),
#         },
#     )
#     post.model_meta["urls"] = {"api": f"/redfin_news/posts/{post.post_id}"}

#     col = get_news_collection()
#     if col is not None:
#         try:
#             col.insert_one(post.dict())  # âœ… body_md/tldrê°€ ê°œë³„ í•„ë“œë¡œ ì €ì¥ë¨
#         except Exception:
#             pass

#     # ì¶œê°„ ì§í›„ ë²¡í„° ì¸ë±ì‹±(ì˜µì…˜)
#     try:
#         if getattr(settings.news, "index_on_publish", False):
#             upsert_news_post_to_chroma(post)
#     except Exception:
#         pass

#     return post


# # --------------------------------------
# # ë°°ì¹˜ ì¶œê°„(ë¡œë” ê¸°ë°˜)
# # --------------------------------------

# def _req_from_loader_doc(doc) -> NewsPublishRequest:
#     md = dict(doc.metadata or {})
#     content = doc.page_content or ""
#     categories: List[str] = []
#     if md.get("content_type"):
#         categories = [str(md["content_type"])]
#     elif md.get("source"):
#         categories = [str(md["source"])]
#     return NewsPublishRequest(
#         article_id=str(md.get("guid")) if md.get("guid") else None,
#         article_code=str(md.get("guid") or md.get("doc_id")) if (md.get("guid") or md.get("doc_id")) else None,
#         url=md.get("url"),
#         title=md.get("title"),
#         content=content,
#         categories=categories,
#         tags=[str(t) for t in (md.get("tags") or [])],
#         publish=True,
#         top_k=6,
#     )


# def publish_from_loader(
#     feed_url: str,
#     field_map: Optional[Dict[str, str]] = None,
#     default_publish: bool = True,
#     default_top_k: int = 6,
#     *,
#     run_config: Optional[Dict[str, Any]] = None,
# ) -> Dict[str, Any]:
#     loader = NewsLoader(field_map=field_map or DEFAULT_FIELD_MAP)
#     docs = loader.load_news_json(feed_url, timeout=15)

#     created, skipped, errors = [], [], []
#     for idx, doc in enumerate(docs):
#         try:
#             md = dict(doc.metadata or {})
#             req = _req_from_loader_doc(doc)
#             if default_publish is not None:
#                 req.publish = bool(default_publish)
#             if default_top_k is not None:
#                 req.top_k = _as_int(default_top_k, 6)
#             q = _dedupe_key({
#                 "article_code": req.article_code,
#                 "article_id": req.article_id,
#                 "url": req.url,
#             }) or ({"article_code": str(md.get("doc_id"))} if md.get("doc_id") else {})
#             if q and _exists_in_news(q):
#                 skipped.append({"index": idx, "reason": "duplicate", "key": q})
#                 continue
#             post = generate_news_post(req, feed_meta=md, run_config=run_config)
#             created.append(post.dict())
#         except Exception as e:
#             errors.append({"index": idx, "error": str(e)})
#     return {"created": created, "skipped": skipped, "errors": errors}


# # --------------------------------------
# # ê¸°ì¡´ JSON ë°°ì—´ ì…ë ¥(ë ˆê±°ì‹œ ìœ ì§€)
# # --------------------------------------

# def publish_batch(
#     items: List[Dict[str, Any]],
#     field_map: Optional[Dict[str, str]] = None,
#     default_publish: bool = True,
#     default_top_k: int = 6,
#     *,
#     run_config: Optional[Dict[str, Any]] = None,
# ) -> Dict[str, Any]:
#     created, skipped, errors = [], [], []
#     m = field_map or {}
#     for idx, it in enumerate(items):
#         try:
#             article_code = str(it.get(m.get("article_code", "article_code")) or it.get("guid") or it.get("id") or "")
#             article_id = str(it.get(m.get("article_id", "article_id")) or it.get("guid") or "")
#             url = it.get(m.get("url", "url")) or it.get("link")
#             title = it.get(m.get("title", "title"))

#             # contentê°€ JSON(ë¬¸ìì—´/ê°ì²´)ì¼ ë•Œ body_mdë§Œ ì¶”ì¶œ
#             raw_content = it.get(m.get("content", "content")) or it.get("article_text")
#             content = None
#             try:
#                 data = raw_content if isinstance(raw_content, (dict, list)) else _safe_json_block(str(raw_content))
#                 if isinstance(data, dict) and "body_md" in data:
#                     content = data.get("body_md")
#             except Exception:
#                 pass
#             if content is None:
#                 content = raw_content or ""

#             categories = it.get(m.get("categories", "categories")) or it.get("content_type")
#             tags = it.get(m.get("tags", "tags")) or []

#             req = NewsPublishRequest(
#                 article_id=article_id or None,
#                 article_code=article_code or None,
#                 url=url,
#                 title=title,
#                 content=content,
#                 categories=[categories] if isinstance(categories, str) else (categories or []),
#                 tags=[str(t) for t in (tags if isinstance(tags, list) else [tags] if tags else [])],
#                 publish=bool(it.get(m.get("publish", "publish"), default_publish)),
#                 top_k=_as_int(it.get(m.get("top_k", "top_k"), default_top_k), 6),
#             )

#             q = _dedupe_key({"article_code": req.article_code, "article_id": req.article_id, "url": req.url})
#             if q and _exists_in_news(q):
#                 skipped.append({"index": idx, "reason": "duplicate", "key": q})
#                 continue

#             post = generate_news_post(req, feed_meta=it, run_config=run_config)
#             created.append(post.dict())
#         except Exception as e:
#             errors.append({"index": idx, "error": str(e)})
#     return {"created": created, "skipped": skipped, "errors": errors}


# # --------------------------------------
# # Mongo â†’ ìš”ì•½/ì¶œê°„(ì¼ê´€ ìŠ¤í‚¤ë§ˆ)
# # --------------------------------------

# def _get_llm_for_news():
#     if not getattr(settings.news, "use_llm", True):
#         return None
#     if os.getenv("GOOGLE_API_KEY"):
#         from langchain_google_genai import ChatGoogleGenerativeAI
#         return ChatGoogleGenerativeAI(model=os.getenv("GEMINI_MODEL", "gemini-2.0-flash"), temperature=0)
#     if os.getenv("OPENAI_API_KEY"):
#         from langchain_openai import ChatOpenAI
#         return ChatOpenAI(model=os.getenv("MODEL_LLM", "gpt-4o-mini"), temperature=0)
#     return None


# def _render_body_md(raw_item: Dict[str, Any], llm, prompt_path: str) -> str:
#     title = raw_item.get("title") or ""
#     content = raw_item.get("article_text") or raw_item.get("content") or ""
#     if not llm:
#         return f"# {title}\n\n{content[:1000]}"

#     try:
#         tpl = Path(prompt_path).read_text(encoding="utf-8") if Path(prompt_path).exists() else ""
#     except Exception:
#         tpl = ""

#     system = "ë‹¹ì‹ ì€ ê¸°ìì…ë‹ˆë‹¤. í•µì‹¬ë§Œ ê°„ê²°íˆ ìš”ì•½í•˜ê³ , ì†Œì œëª©ê³¼ ë¶ˆë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
#     user = f"{tpl}\n\n[ì œëª©]\n{title}\n\n[ë³¸ë¬¸]\n{content}\n\nìœ„ ë‚´ìš©ì„ ê°„ê²°í•œ ê¸°ì‚¬ ìš”ì•½(Markdown)ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."
#     try:
#         resp = llm.invoke([("system", system), ("user", user)])
#         text = getattr(resp, "content", None) or str(resp)
#         return text.strip()
#     except Exception as e:  # pragma: no cover (ë„¤íŠ¸ì›Œí¬/ëª¨ë¸ ì´ìŠˆ)
#         return f"# {title}\n\n{content[:1000]}\n\n<!-- LLM ì‹¤íŒ¨: {e} -->"


# def publish_from_source(*, mongo_query: Optional[Dict[str, Any]] = None, limit: int = 30) -> Dict[str, Any]:
#     """Mongo redfin.extract â†’ ìš”ì•½ â†’ redfin.news_logs(NewsPost ìŠ¤í‚¤ë§ˆ) upsert.
#     - âœ¨ body_md/tldrê°€ ê°œë³„ í•„ë“œë¡œ ì €ì¥ë¨(JSON ë¬¸ìì—´ ê¸ˆì§€)
#     """
#     loader = NewsLoader()
#     docs = loader.load_news_mongo(
#         uri=settings.mongo.uri,
#         db=settings.mongo.db,
#         collection=getattr(settings.news, "source_collection", "extract"),
#         query=mongo_query or {},
#         limit=limit,
#         timeout_ms=getattr(settings.mongo, "timeout_ms", 3000),
#         sort=[("processed_at", -1), ("published", -1)],
#     )
#     if not docs:
#         return {"ok": False, "reason": "no input docs from mongo.extract"}
#     # ğŸ”§ FIX: NameError ë°©ì§€ â€” í…œí”Œë¦¿ ê²½ë¡œ ì •ì˜
#     tpl_path = getattr(settings.news, "prompt_path", "src/prompts/templates/news_publish_v1.md")

#     llm = _get_llm_for_news()
#     client = MongoClient(settings.mongo.uri, serverSelectionTimeoutMS=settings.mongo.timeout_ms)
#     coll = client[settings.mongo.db][getattr(settings.mongo, "news_collection", "news_logs")]

#     ops: List[UpdateOne] = []
#     count = 0
#     for d in docs:
#         md = d.metadata or {}
#         raw = {
#             "title": md.get("title"),
#             "article_text": d.page_content,  # ì›ë¬¸
#             "source": md.get("source"),
#             "link": md.get("url") or md.get("link"),
#             "guid": md.get("guid"),
#             "published_at": md.get("published"),
#             "tags": md.get("tags") or [],
#             "authors": md.get("authors") or [],
#             "language": md.get("lang"),
#         }
#         body_md = _render_body_md(raw, llm, prompt_path=tpl_path)
#         tldr_list = _fallback_tldr_from_text(body_md, 3)

#         # ì¼ê´€ ìŠ¤í‚¤ë§ˆë¡œ upsert
#         doc = {
#             "type": "post",
#             "title": raw["title"],
#             "dek": None,
#             "tldr": tldr_list,
#             "body_md": body_md,
#             "hero_image_url": None,
#             "author_name": None,
#             "category": None,
#             "categories": [],
#             "tags": raw["tags"],
#             "sources": [],
#             "status": "published",
#             "model_meta": {
#                 "persona": "news_brief",
#                 "strategy": "auto",
#                 "feed": {k: v for k, v in md.items() if k not in {"article_text"}},
#                 "service": getattr(settings.news, "service_name", "redfin_news"),
#                 "ls_project": getattr(settings.news, "langsmith_project", None),
#             },
#             "source": raw["source"],
#             "link": raw["link"],
#             "guid": raw["guid"],
#             "published_at": md.get("published") or md.get("published_at"),
#             "lang": raw["language"],
#             "created_at": datetime.utcnow(),
#             "updated_at": datetime.utcnow(),
#         }
#         filt = {"$or": [{"guid": raw["guid"]}, {"link": raw["link"]}]}
#         ops.append(UpdateOne(filt, {"$set": doc}, upsert=True))
#         count += 1

#     if not ops:
#         return {"ok": True, "count": 0, "upserts": 0, "modified": 0}

#     res = coll.bulk_write(ops, ordered=False)
#     upserts = len(res.upserted_ids or {})
#     modified = res.modified_count
#     return {"ok": True, "count": count, "upserts": upserts, "modified": modified}


# # --------------------------------------
# # ì™¸ë¶€ ì§„ì…ì (ë¼ìš°í„°ì—ì„œ ì‚¬ìš©)
# # --------------------------------------

# def publish_from_feed(
#     feed_url: str,
#     item_path: Optional[str] = None,  # ë³´ì¡´(ë¯¸ì‚¬ìš©)
#     field_map: Optional[Dict[str, str]] = None,
#     default_publish: bool = True,
#     default_top_k: int = 6,
#     *,
#     run_config: Optional[Dict[str, Any]] = None,
# ) -> Dict[str, Any]:
#     return publish_from_loader(
#         feed_url=feed_url,
#         field_map=field_map,
#         default_publish=default_publish,
#         default_top_k=default_top_k,
#         run_config=run_config,
#     )


# def publish_from_env(*, run_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
#     feed_url = settings.news.api_url or os.getenv("NEWS_API_URL")
#     if not feed_url:
#         raise ValueError("NEWS_API_URL/settings.news.api_url is empty")

#     user_map = getattr(settings.news, "feed_field_map", None)
#     if user_map is None:
#         field_map_json = os.getenv("NEWS_FEED_FIELD_MAP")
#         user_map = json.loads(field_map_json) if field_map_json else None
#     field_map = {**DEFAULT_FIELD_MAP, **user_map} if user_map else DEFAULT_FIELD_MAP

#     default_publish = _as_bool(os.getenv("NEWS_DEFAULT_PUBLISH"),
#                                default=_as_bool(getattr(settings.news, "default_publish", True), True))
#     default_top_k = _as_int(os.getenv("NEWS_TOP_K"),
#                             default=_as_int(getattr(settings.news, "top_k", 6), 6))

#     return publish_from_loader(
#         feed_url=feed_url,
#         field_map=field_map,
#         default_publish=default_publish,
#         default_top_k=default_top_k,
#         run_config=run_config,
#     )


# # -*- coding: utf-8 -*-
# # =============================================================================
# # ë³€ê²½ ìš”ì•½ (2025-09-04)
# # 1) [CHG] íŒŒì„œ ê°•í™”:
# #    - _coerce_text / _safe_json_block / _parse_llm_output_any ì¶”ê°€
# #    - ë¬¸ìì—´/AIMessage/dict ì–´ë–¤ ê²°ê³¼ë¼ë„ ì¼ê´€ë˜ê²Œ íŒŒì‹±
# # 2) [CHG] generate_news_post():
# #    - LLM ë¯¸ì‚¬ìš©ì‹œ ë¶ˆí•„ìš”í•œ JSON í•©ì„± ì œê±°
# #    - ì²´ì¸ ê²°ê³¼ë¥¼ ì›ì‹œ í…ìŠ¤íŠ¸í™” â†’ JSONì´ë©´ NewsLLMOutë¡œ íŒŒì‹±
# #    - íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ body_mdë§Œ ì•ˆì „ ì¶”ì¶œ(ì›ë¬¸/ë§ˆí¬ë‹¤ìš´ ìœ ì§€)
# # 3) [CHG] publish_batch():
# #    - ì…ë ¥ contentê°€ JSON(ë¬¸ìì—´/ê°ì²´)ì¼ ë•Œ body_mdë§Œ ë½‘ì•„ ì‚¬ìš©
# # 4) ê¸°ëŠ¥/ê¸°ì¡´ ë¡œì§ì€ ìœ ì§€(ì¤‘ë³µíŒì •, ì¸ë±ì‹±, ì»¬ë ‰ì…˜ ì €ì¥ ë“±)
# # =============================================================================
# from __future__ import annotations
# import os
# import json
# import urllib.request
# import re
# from uuid import uuid4
# from typing import Any, Dict, Optional, List
# from datetime import datetime
# from pymongo import MongoClient, UpdateOne

# from schemas.news import NewsPublishRequest, NewsPost
# from schemas.news_llm import NewsLLMOut   # [CHG] ê´€ëŒ€í•œ ìŠ¤í‚¤ë§ˆë¡œ êµì²´ë¨
# from observability.mongo_logger import get_news_collection
# from nureongi.loaders import NewsLoader, DEFAULT_FIELD_MAP
# from nureongi.vs_news import upsert_news_post_to_chroma
# from nureongi.vectorstore import create_chroma_store

# # ë…ë¦½ LLM/ì²´ì¸
# from nureongi.llm import build_default_llm
# from nureongi.news_chain import build_news_chain

# # í…œí”Œë¦¿/ì„¸íŒ…
# from core.settings import settings
# from nureongi.prompt_loader import load_md_template, render_template

# # ===== (ì‹œë§¨í‹± ì¸ë±ìŠ¤ìš©) ì„ë² ë”©/ì²­í‚¹ =====
# try:
#     from langchain_huggingface import HuggingFaceEmbeddings
#     from langchain_experimental.text_splitter import SemanticChunker
#     from langchain_core.documents import Document  # íƒ€ì… íŒíŠ¸ ìš©ë„
# except Exception:
#     HuggingFaceEmbeddings = None
#     SemanticChunker = None
#     Document = None

# # RecursiveCharacterTextSplitter ì„í¬íŠ¸ (ë²„ì „ì— ë”°ë¥¸ í´ë°± ì²˜ë¦¬)
# try:
#     from langchain_text_splitters import RecursiveCharacterTextSplitter
# except Exception:
#     from langchain.text_splitter import RecursiveCharacterTextSplitter

# from langchain_huggingface import HuggingFaceEmbeddings  # ì„ë² ë”© í•„ìš”


# # ------------------- ìœ í‹¸/í˜•ë³€í™˜ -------------------

# def _as_bool(v: object | None, default: bool = False) -> bool:
#     if isinstance(v, bool):
#         return v
#     if v is None:
#         return default
#     return str(v).strip().lower() in ("1", "true", "yes", "on")

# def _as_int(v: object | None, default: int = 0) -> int:
#     try:
#         if v is None:
#             return default
#         s = str(v).strip()
#         if s == "":
#             return default
#         return int(s)
#     except Exception:
#         return default

# # [CHG] LLM/ì²´ì¸ ë°˜í™˜ì„ í•­ìƒ strë¡œ ì •ê·œí™”
# def _coerce_text(x: Any) -> str:
#     try:
#         if hasattr(x, "content"):
#             return str(getattr(x, "content"))
#         if isinstance(x, (dict, list)):
#             return json.dumps(x, ensure_ascii=False)
#         return str(x)
#     except Exception:
#         return str(x)

# # [CHG] ë¬¸ìì—´ì—ì„œ JSON ë¸”ë¡ë§Œ ì˜ë¼ íŒŒì‹±
# def _safe_json_block(s: str) -> Dict[str, Any]:
#     s = (s or "").strip()
#     i, j = s.find("{"), s.rfind("}")
#     if i != -1 and j != -1 and j > i:
#         s = s[i : j + 1]
#     return json.loads(s)

# # [CHG] ì–´ë–¤ ê²°ê³¼ë“  NewsLLMOutë¡œ íŒŒì‹± ì‹œë„
# def _parse_llm_output_any(x: Any) -> Optional[NewsLLMOut]:
#     try:
#         if isinstance(x, dict):
#             data = x
#         else:
#             s = _coerce_text(x)
#             data = _safe_json_block(s)
#     except Exception:
#         return None

#     try:
#         if hasattr(NewsLLMOut, "model_validate"):   # pydantic v2
#             return NewsLLMOut.model_validate(data)
#         return NewsLLMOut.parse_obj(data)           # pydantic v1
#     except Exception:
#         return None

# def _listify(v: Any) -> List[str]:
#     if v is None:
#         return []
#     if isinstance(v, list):
#         return [str(x) for x in v if x is not None]
#     return [str(v)]

# def _dedupe_key(item: Dict[str, Any]) -> Dict[str, Any]:
#     q: Dict[str, Any] = {}
#     if item.get("article_code"):
#         q["article_code"] = str(item["article_code"])
#     elif item.get("article_id"):
#         q["article_id"] = str(item["article_id"])
#     elif item.get("url"):
#         q["url"] = str(item["url"])
#     return q

# def _exists_in_news(q: Dict[str, Any]) -> bool:
#     if not q:
#         return False
#     col = get_news_collection()
#     if col is None:
#         return False
#     return col.find_one(q) is not None


# # -------------------- ì´ˆê¸°í™” í•¨ìˆ˜(ìˆ˜ëª…ì£¼ê¸°ì—ì„œ í˜¸ì¶œ) --------------------

# def init_index() -> dict:
#     """ë‰´ìŠ¤ìš© Chroma ì»¬ë ‰ì…˜ ìƒì„±/ì—´ê¸°."""
#     try:
#         vs = create_chroma_store(
#             collection_name=settings.news.collection,
#             persist_dir=settings.news.persist_dir,
#             distance="cosine",
#         )
#         vs.persist()
#         return {
#             "ok": True,
#             "collection": settings.news.collection,
#             "persist_dir": settings.news.persist_dir,
#         }
#     except Exception as e:
#         return {"ok": False, "error": str(e)}

# def init_news_index_semantic() -> dict:
#     """
#     ë‰´ìŠ¤ ì „ìš© 'ì‹œë§¨í‹± ì¸ë±ìŠ¤' ì´ˆê¸°í™”:
#     - settings.news.api_urlì—ì„œ ê¸°ì‚¬ ë¡œë“œ â†’ ì‹œë§¨í‹± ì²­í‚¹ â†’ Chroma ì—…ì„œíŠ¸
#     """
#     try:
#         if not settings.news.api_url:
#             return {"ok": False, "reason": "settings.news.api_url is empty"}
#         if HuggingFaceEmbeddings is None or SemanticChunker is None:
#             return {"ok": False, "reason": "semantic stack unavailable (langchain_experimental / huggingface not installed)"}

#         loader = NewsLoader(field_map=DEFAULT_FIELD_MAP)
#         docs = loader.load_news_json(settings.news.api_url, timeout=15)
#         if not docs:
#             return {"ok": False, "reason": "no documents loaded", "api_url": settings.news.api_url}

#         embed_for_chunk = HuggingFaceEmbeddings(model_name=settings.news.emb_model)
#         splitter = SemanticChunker(embeddings=embed_for_chunk, breakpoint_threshold_type="percentile")
#         chunks = splitter.split_documents(docs)

#         vs = create_chroma_store(
#             collection_name=settings.news.collection,
#             persist_dir=settings.news.persist_dir,
#             distance="cosine",
#         )
#         vs.vs.add_documents(chunks)
#         vs.persist()

#         return {
#             "ok": True,
#             "api_url": settings.news.api_url,
#             "collection": settings.news.collection,
#             "persist_dir": settings.news.persist_dir,
#             "docs": len(docs),
#             "chunks": len(chunks),
#         }
#     except Exception as e:
#         return {"ok": False, "error": str(e)}


# # -------------------- ë‹¨ê±´ ìƒì„± --------------------

# def generate_news_post(
#     req: NewsPublishRequest,
#     feed_meta: Optional[Dict[str, Any]] = None,
#     *,
#     run_config: Optional[Dict[str, Any]] = None,   # ìš”ì²­ ë‹¨ìœ„ íŠ¸ë ˆì´ìŠ¤ ì„¤ì •
# ) -> NewsPost:
#     """
#     .md í…œí”Œë¦¿ì„ ë¡œë“œí•˜ì—¬ LLMì— ì „ë‹¬.
#     - retriever ì—†ìŒ(ë‰´ìŠ¤ ë‹¨ê±´ ìš”ì•½/ê°€ê³µ)
#     - ë¼ìš°í„°ì—ì„œ ë°›ì€ run_configë¥¼ ì²´ì¸ invokeì— ê·¸ëŒ€ë¡œ ì „ë‹¬
#     """
#     # 1) í…œí”Œë¦¿ ë¡œë”©/ë Œë”ë§
#     tpl = load_md_template(settings.news.prompt_path or "src/prompts/templates/news_publish_v1.md")

#     # ì•ˆì „ì¥ì¹˜: ì¸ì‚¬ì´íŠ¸ í…œí”Œë¦¿ ì„ì„ ì°¨ë‹¨
#     _tpl_str = str(tpl)
#     if ("Issue List" in _tpl_str) or ("target-insight" in _tpl_str):
#         raise RuntimeError("ë‰´ìŠ¤ ì¶œê°„ì—ì„œ ì¸ì‚¬ì´íŠ¸ í…œí”Œë¦¿ì´ ê°ì§€ë¨. settings.news.prompt_pathë¥¼ í™•ì¸í•˜ì„¸ìš”.")

#     prompt = render_template(
#         tpl,
#         title=(req.title or ""),
#         url=(req.url or ""),
#         content=(req.content or ""),
#         categories=", ".join(req.categories or []),
#         tags=", ".join(req.tags or []),
#         meta=json.dumps(feed_meta or {}, ensure_ascii=False),
#     )

#     # 2) ìš”ì•½/ê°€ê³µ
#     # [CHG] LLM ë¯¸ì‚¬ìš©ì‹œ ë¶ˆí•„ìš”í•œ JSON í•©ì„± ì œê±°
#     if not settings.news.use_llm:
#         raw_text = req.content or ""
#         llm_out = None
#     else:
#         llm = build_default_llm()
#         chain = build_news_chain(llm, settings.news.prompt_path or "src/prompts/templates/news_publish_v1.md")
#         inputs = {
#             "title": (req.title or ""),
#             "url": (req.url or ""),
#             "content": (req.content or ""),
#             "categories": ", ".join(req.categories or []),
#             "tags": ", ".join(req.tags or []),
#             "meta": json.dumps(feed_meta or {}, ensure_ascii=False),
#         }

#         if run_config is None or not isinstance(run_config, dict) or not run_config.get("callbacks"):
#             print("[warn] news run_config.callbacks missing; LangSmith project may fallback to env")
#         else:
#             try:
#                 cb = run_config["callbacks"][0]
#                 print("[diag] news callback tracer =", type(cb).__name__, getattr(cb, "project_name", None))
#             except Exception:
#                 pass

#         # [CHG] ê²°ê³¼ë¥¼ ë¨¼ì € ì›ì‹œ í…ìŠ¤íŠ¸ë¡œ ì •ê·œí™” â†’ JSON íŒŒì‹± ì‹œë„
#         raw_text = _coerce_text(chain.invoke(inputs, config=(run_config or {})))
#         llm_out = _parse_llm_output_any(raw_text)

#     # [CHG] ìµœì¢… í•„ë“œ ê²°ì •: JSON êµ¬ì¡°ê°€ ìˆìœ¼ë©´ ìš°ì„ , ì—†ìœ¼ë©´ raw_text/req.contentë¥¼ ë§ˆí¬ë‹¤ìš´ ë³¸ë¬¸ìœ¼ë¡œ ì‚¬ìš©
#     if llm_out:
#         title = llm_out.title or (req.title or "Untitled")
#         body_md = llm_out.body_md or (req.content or "")
#         tldr_list = llm_out.tldr or _fallback_tldr_from_text(body_md, k=3)
#         tags = llm_out.tags or (req.tags or [])
#         sources = llm_out.sources or []
#         hero = llm_out.hero_image_url or None
#         subtitle = llm_out.subtitle or None
#         author = llm_out.author_name or getattr(req, "author_name", None)
#     else:
#         title = req.title or "Untitled"
#         body_md = req.content or ""
#         # raw_textê°€ JSONì´ë©´ body_mdë§Œ ì¶”ì¶œ ì‹œë„
#         try:
#             maybe = _safe_json_block(raw_text)
#             if isinstance(maybe, dict) and "body_md" in maybe:
#                 body_md = str(maybe["body_md"] or body_md)
#         except Exception:
#             if raw_text.strip():
#                 body_md = raw_text
#         tldr_list = _fallback_tldr_from_text(body_md, k=3)
#         tags, sources, hero, subtitle = (req.tags or []), [], None, None
#         author = getattr(req, "author_name", None)

#     category1 = (req.categories[0] if req.categories else None)

#     post = NewsPost(
#         post_id=str(uuid4()),
#         article_id=req.article_id,
#         article_code=getattr(req, "article_code", None),
#         url=req.url,
#         title=title,
#         dek=subtitle,
#         tldr=tldr_list,
#         body_md=body_md,
#         hero_image_url=hero,
#         author_name=author,
#         category=category1,
#         categories=req.categories or [],
#         tags=tags,
#         sources=sources,
#         status="published" if req.publish else "draft",
#         model_meta={
#             "persona": "news_brief",
#             "strategy": "auto",
#             "top_k": req.top_k,
#             "feed": feed_meta or {},
#             "parsed_ok": bool(llm_out is not None),
#             "service": getattr(settings.news, "service_name", "redfin_news"),
#             "ls_project": getattr(settings.news, "langsmith_project", None),
#         },
#     )
#     post.model_meta["urls"] = {"api": f"/redfin_news/posts/{post.post_id}"}

#     col = get_news_collection()
#     if col is not None:
#         try:
#             col.insert_one(post.dict())
#         except Exception:
#             pass

#     # ì¶œê°„ ì§í›„ ìë™ ì¸ë±ì‹±
#     try:
#         if getattr(settings.news, "index_on_publish", False):
#             upsert_news_post_to_chroma(post)
#     except Exception:
#         pass

#     return post


# # -------------------- NewsLoader ê¸°ë°˜ ë°°ì¹˜ ì¶œê°„ --------------------

# def _req_from_loader_doc(doc) -> NewsPublishRequest:
#     md = dict(doc.metadata or {})
#     content = doc.page_content or ""
#     categories: List[str] = []
#     if md.get("content_type"):
#         categories = [str(md["content_type"])]
#     elif md.get("source"):
#         categories = [str(md["source"])]
#     return NewsPublishRequest(
#         article_id=str(md.get("guid")) if md.get("guid") else None,
#         article_code=str(md.get("guid") or md.get("doc_id")) if (md.get("guid") or md.get("doc_id")) else None,
#         url=md.get("url"),
#         title=md.get("title"),
#         content=content,
#         categories=categories,
#         tags=[str(t) for t in (md.get("tags") or [])],
#         publish=True,
#         top_k=6,
#     )

# def publish_from_loader(
#     feed_url: str,
#     field_map: Optional[Dict[str, str]] = None,
#     default_publish: bool = True,
#     default_top_k: int = 6,
#     *,
#     run_config: Optional[Dict[str, Any]] = None,
# ) -> Dict[str, Any]:
#     loader = NewsLoader(field_map=field_map or DEFAULT_FIELD_MAP)
#     docs = loader.load_news_json(feed_url, timeout=timeout if (timeout := 15) else 15)

#     created, skipped, errors = [], [], []
#     for idx, doc in enumerate(docs):
#         try:
#             md = dict(doc.metadata or {})
#             req = _req_from_loader_doc(doc)

#             if default_publish is not None:
#                 req.publish = bool(default_publish)
#             if default_top_k is not None:
#                 try:
#                     req.top_k = int(default_top_k)
#                 except Exception:
#                     req.top_k = 6

#             q = _dedupe_key({
#                 "article_code": req.article_code,
#                 "article_id": req.article_id,
#                 "url": req.url,
#             }) or ({"article_code": str(md.get("doc_id"))} if md.get("doc_id") else {})
#             if q and _exists_in_news(q):
#                 skipped.append({"index": idx, "reason": "duplicate", "key": q})
#                 continue

#             post = generate_news_post(req, feed_meta=md, run_config=run_config)
#             created.append(post.dict())
#         except Exception as e:
#             errors.append({"index": idx, "error": str(e)})
#     return {"created": created, "skipped": skipped, "errors": errors}


# # -------------------- ê¸°ì¡´ JSON ë°°ì—´ ë°°ì¹˜(ì˜µì…˜ìœ¼ë¡œ ìœ ì§€) --------------------

# def publish_batch(
#     items: List[Dict[str, Any]],
#     field_map: Optional[Dict[str, str]] = None,
#     default_publish: bool = True,
#     default_top_k: int = 6,
#     *,
#     run_config: Optional[Dict[str, Any]] = None,
# ) -> Dict[str, Any]:
#     created, skipped, errors = [], [], []
#     m = field_map or {}
#     for idx, it in enumerate(items):
#         try:
#             article_code = str(it.get(m.get("article_code", "article_code")) or it.get("guid") or it.get("id") or "")
#             article_id = str(it.get(m.get("article_id", "article_id")) or it.get("guid") or "")
#             url = it.get(m.get("url", "url")) or it.get("link")
#             title = it.get(m.get("title", "title"))

#             # [CHG] contentê°€ JSON(ë¬¸ìì—´/ê°ì²´)ì¼ ë•Œ body_mdë§Œ ì¶”ì¶œ
#             raw_content = it.get(m.get("content", "content")) or it.get("article_text")
#             content = None
#             try:
#                 if isinstance(raw_content, (dict, list)):
#                     data = raw_content
#                 else:
#                     data = _safe_json_block(str(raw_content))
#                 if isinstance(data, dict) and "body_md" in data:
#                     content = data.get("body_md")
#             except Exception:
#                 pass
#             if content is None:
#                 content = raw_content or ""

#             categories = it.get(m.get("categories", "categories")) or it.get("content_type")
#             tags = it.get(m.get("tags", "tags")) or []

#             req = NewsPublishRequest(
#                 article_id=article_id or None,
#                 article_code=article_code or None,
#                 url=url,
#                 title=title,
#                 content=content,
#                 categories=[categories] if isinstance(categories, str) else (categories or []),
#                 tags=[str(t) for t in (tags if isinstance(tags, list) else [tags] if tags else [])],
#                 publish=bool(it.get(m.get("publish", "publish"), default_publish)),
#                 top_k=int(it.get(m.get("top_k", "top_k"), default_top_k)),
#             )

#             q = _dedupe_key({"article_code": req.article_code, "article_id": req.article_id, "url": req.url})
#             if q and _exists_in_news(q):
#                 skipped.append({"index": idx, "reason": "duplicate", "key": q})
#                 continue

#             post = generate_news_post(req, feed_meta=it, run_config=run_config)
#             created.append(post.dict())
#         except Exception as e:
#             errors.append({"index": idx, "error": str(e)})
#     return {"created": created, "skipped": skipped, "errors": errors}


# # -------------------- ì™¸ë¶€ í˜¸ì¶œ ì§„ì…ì  --------------------

# def publish_from_feed(
#     feed_url: str,
#     item_path: Optional[str] = None,      # ë³´ì¡´(ë¯¸ì‚¬ìš©)
#     field_map: Optional[Dict[str, str]] = None,
#     default_publish: bool = True,
#     default_top_k: int = 6,
#     *,
#     run_config: Optional[Dict[str, Any]] = None,
# ) -> Dict[str, Any]:
#     return publish_from_loader(
#         feed_url=feed_url,
#         field_map=field_map,
#         default_publish=default_publish,
#         default_top_k=default_top_k,
#         timeout=15,
#         run_config=run_config,
#     )

# def publish_from_env(*, run_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
#     feed_url = settings.news.api_url or os.getenv("NEWS_API_URL")
#     if not feed_url:
#         raise ValueError("NEWS_API_URL/settings.news.api_url is empty")

#     user_map = getattr(settings.news, "feed_field_map", None)
#     if user_map is None:
#         field_map_json = os.getenv("NEWS_FEED_FIELD_MAP")
#         user_map = json.loads(field_map_json) if field_map_json else None
#     field_map = {**DEFAULT_FIELD_MAP, **user_map} if user_map else DEFAULT_FIELD_MAP

#     default_publish = _as_bool(os.getenv("NEWS_DEFAULT_PUBLISH", None),
#                                default=_as_bool(getattr(settings.news, "default_publish", True), True))
#     default_top_k = _as_int(os.getenv("NEWS_TOP_K", None),
#                             default=_as_int(getattr(settings.news, "top_k", 6), 6))

#     return publish_from_loader(
#         feed_url=feed_url,
#         field_map=field_map,
#         default_publish=default_publish,
#         default_top_k=default_top_k,
#         timeout=15,
#         run_config=run_config,
#     )


# # ------------------- í—¬í¼ -------------------

# def _fallback_tldr_from_text(text: str, k: int = 3) -> List[str]:
#     sents = re.split(r"(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+", (text or "").strip())
#     out = []
#     for s in sents:
#         s = s.strip()
#         if not s:
#             continue
#         out.append(s[:90])
#         if len(out) >= max(1, k):
#             break
#     return out


# # ------------------- ê³ ì •í¬ê¸° ì²­í‚¹ + ì˜¤ë²„ë© ì¸ë±ìŠ¤ ì´ˆê¸°í™” -------------------

# def init_news_index_fixed(
#     *,
#     chunk_size: int = 1200,
#     chunk_overlap: int = 120,
# ) -> dict:
#     try:
#         if not settings.news.api_url:
#             return {"ok": False, "reason": "settings.news.api_url is empty"}

#         loader = NewsLoader(field_map=DEFAULT_FIELD_MAP)
#         docs = loader.load_news_json(settings.news.api_url, timeout=15)
#         if not docs:
#             return {"ok": False, "reason": "no documents loaded", "api_url": settings.news.api_url}

#         splitter = RecursiveCharacterTextSplitter(
#             chunk_size=chunk_size,
#             chunk_overlap=chunk_overlap,
#             separators=["\n\n", "\n", ". ", "? ", "! ", ", ", " ", ""],
#         )
#         chunks = splitter.split_documents(docs)

#         embedding = HuggingFaceEmbeddings(model_name=settings.news.emb_model)

#         vs = create_chroma_store(
#             embedding=embedding,
#             collection_name=settings.news.collection,
#             persist_dir=settings.news.persist_dir,
#             distance="cosine",
#         )
#         vs.vs.add_documents(chunks)
#         vs.persist()

#         return {
#             "ok": True,
#             "mode": "fixed",
#             "api_url": settings.news.api_url,
#             "collection": settings.news.collection,
#             "persist_dir": settings.news.persist_dir,
#             "docs": len(docs),
#             "chunks": len(chunks),
#             "chunk_size": chunk_size,
#             "chunk_overlap": chunk_overlap,
#         }
#     except Exception as e:
#         return {"ok": False, "error": str(e)}


# # ------------------- Mongo â†’ ë‰´ìŠ¤ ìš”ì•½/ì¶œê°„ (ì˜µì…˜) -------------------

# def _get_llm_for_news():
#     """ì¶œê°„ìš© LLM. NEWS__USE_LLM=falseë©´ None ë°˜í™˜í•´ì„œ íŒ¨ìŠ¤ìŠ¤ë£¨ ì €ì¥."""
#     from core import settings
#     if not getattr(settings.news, "use_llm", True):
#         return None
#     import os
#     if os.getenv("GOOGLE_API_KEY"):
#         from langchain_google_genai import ChatGoogleGenerativeAI
#         return ChatGoogleGenerativeAI(model=os.getenv("GEMINI_MODEL","gemini-2.0-flash"), temperature=0)
#     if os.getenv("OPENAI_API_KEY"):
#         from langchain_openai import ChatOpenAI
#         return ChatOpenAI(model=os.getenv("MODEL_LLM","gpt-4o-mini"), temperature=0)
#     return None

# def _render_body_md(raw_item: Dict[str, Any], llm, prompt_path: str) -> str:
#     """í…œí”Œë¦¿+LLMë¡œ ìš”ì•½ ë³¸ë¬¸ ìƒì„±. LLMì´ ì—†ìœ¼ë©´ íŒ¨ìŠ¤ìŠ¤ë£¨(ê¸°ì‚¬ ë³¸ë¬¸ ì¼ë¶€/ì œëª©ë§Œ)."""
#     title = raw_item.get("title") or ""
#     content = raw_item.get("article_text") or raw_item.get("content") or ""
#     if not llm:
#         return f"# {title}\n\n{content[:800]}"

#     from pathlib import Path
#     tpl = ""
#     try:
#         p = Path(prompt_path)
#         tpl = p.read_text(encoding="utf-8") if p.exists() else ""
#     except Exception:
#         tpl = ""

#     system = "ë‹¹ì‹ ì€ ê¸°ìì…ë‹ˆë‹¤. í•µì‹¬ë§Œ ê°„ê²°íˆ ìš”ì•½í•˜ê³ , ì†Œì œëª©ê³¼ ë¶ˆë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
#     user = f"{tpl}\n\n[ì œëª©]\n{title}\n\n[ë³¸ë¬¸]\n{content}\n\nìœ„ ë‚´ìš©ì„ ê°„ê²°í•œ ê¸°ì‚¬ ìš”ì•½(Markdown)ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."
#     try:
#         resp = llm.invoke([("system", system), ("user", user)])
#         text = getattr(resp, "content", None) or str(resp)
#         return text.strip()
#     except Exception as e:
#         return f"# {title}\n\n{content[:800]}\n\n<!-- LLM ì‹¤íŒ¨: {e} -->"

# def publish_from_source(*, mongo_query: Optional[Dict[str, Any]] = None, limit: int = 30) -> Dict[str, Any]:
#     """
#     Mongo redfin.extract ì—ì„œ ê¸°ì‚¬ ë¡œë“œ â†’ (ì„ íƒ)ìš”ì•½ â†’ redfin.news_logsì— upsert ì €ì¥.
#     - retriever ì‚¬ìš© ì—†ìŒ
#     - api_url ì˜ì¡´ ì—†ìŒ
#     """
#     from core import settings
#     from nureongi.loaders import NewsLoader

#     loader = NewsLoader()
#     docs = loader.load_news_mongo(
#         uri=settings.mongo.uri,
#         db=settings.mongo.db,
#         collection=getattr(settings.news, "source_collection", "extract"),
#         query=mongo_query or {},
#         limit=limit,
#         timeout_ms=getattr(settings.mongo, "timeout_ms", 3000),
#         sort=[("processed_at", -1), ("published", -1)],
#     )
#     if not docs:
#         return {"ok": False, "reason": "no input docs from mongo.extract"}

#     llm = _get_llm_for_news()
#     posts: List[Dict[str, Any]] = []
#     for d in docs:
#         md = d.metadata or {}
#         raw = {
#             "title": md.get("title"),
#             "article_text": d.page_content,
#             "source": md.get("source"),
#             "link": md.get("url") or md.get("link"),
#             "guid": md.get("guid"),
#             "published_at": md.get("published_at"),
#             "tags": md.get("tags") or [],
#             "authors": md.get("authors") or [],
#             "language": md.get("lang"),
#         }
#         body_md = _render_body_md(raw, llm, prompt_path=getattr(settings.news, "prompt_path", "src/prompts/templates/news_publish_v1.md"))
#         posts.append({
#             "type": "post",
#             "title": raw["title"],
#             "body_md": body_md,
#             "source": raw["source"],
#             "link": raw["link"],
#             "guid": raw["guid"],
#             "published_at": raw["published_at"],
#             "tags": raw["tags"],
#             "authors": raw["authors"],
#             "lang": raw["language"],
#             "created_at": datetime.utcnow(),
#             "updated_at": datetime.utcnow(),
#         })

#     client = MongoClient(settings.mongo.uri, serverSelectionTimeoutMS=settings.mongo.timeout_ms)
#     coll = client[settings.mongo.db][getattr(settings.mongo, "news_collection", "news_logs")]

#     bulk: List[UpdateOne] = []
#     for p in posts:
#         filt = {"$or": [{"guid": p.get("guid")}, {"link": p.get("link")}]}
#         bulk.append(UpdateOne(filt, {"$set": p}, upsert=True))

#     if bulk:
#         res = coll.bulk_write(bulk, ordered=False)
#         upserts = len(res.upserted_ids or {})
#         modified = res.modified_count
#     else:
#         upserts = modified = 0

#     return {"ok": True, "count": len(posts), "upserts": upserts, "modified": modified}

######################################################################################################################

# src/services/news_service.py
# ----------------------------------------------------------------------
# ë³€ê²½ ìš”ì•½
# - ë¼ìš°í„°ì—ì„œ ì „ë‹¬í•œ run_config(=callbacks/tags/metadata í¬í•¨)ë¥¼
#   ì²´ì¸/LLM í˜¸ì¶œì— ê·¸ëŒ€ë¡œ ì „ë‹¬í•˜ë„ë¡ ì‹œê·¸ë‹ˆì²˜ í™•ì¥.
# - env ìŠ¤ì™‘ ì»¨í…ìŠ¤íŠ¸(langsmith_project) ì œê±°. (ë™ì‹œì„± ì•ˆì „)
# - ë‚˜ë¨¸ì§€ í¼ë¸”ë¦¬ì‹œ í”Œë¡œìš°/ì¤‘ë³µ íŒì •/Loader ì—°ê³„ëŠ” ê¸°ì¡´ ìœ ì§€.
# ----------------------------------------------------------------------
from __future__ import annotations
import os
import json
import urllib.request
import re
from uuid import uuid4
from typing import Any, Dict, Optional, List
from datetime import datetime
from pymongo import MongoClient, UpdateOne

from schemas.news import NewsPublishRequest, NewsPost
from schemas.news_llm import NewsLLMOut
from observability.mongo_logger import get_news_collection
from nureongi.loaders import NewsLoader, DEFAULT_FIELD_MAP
from nureongi.vs_news import upsert_news_post_to_chroma
from nureongi.vectorstore import create_chroma_store

# ë…ë¦½ LLM/ì²´ì¸
from nureongi.llm import build_default_llm
from nureongi.news_chain import build_news_chain

# í…œí”Œë¦¿
from core.settings import settings
from nureongi.prompt_loader import load_md_template, render_template

# ===== (ì‹œë§¨í‹± ì¸ë±ìŠ¤ìš©) ì„ë² ë”©/ì²­í‚¹ =====
try:
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_experimental.text_splitter import SemanticChunker
    from langchain_core.documents import Document  # íƒ€ì… íŒíŠ¸ ìš©ë„
except Exception:
    HuggingFaceEmbeddings = None
    SemanticChunker = None
    Document = None

# RecursiveCharacterTextSplitter ì„í¬íŠ¸ (ë²„ì „ì— ë”°ë¥¸ í´ë°± ì²˜ë¦¬)
try:
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except Exception:
    from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain_huggingface import HuggingFaceEmbeddings  # ì„ë² ë”© í•„ìš”


# (ì˜µì…˜) ë¬¸ìì—´â†’ë¶ˆë¦¬ì–¸/ì •ìˆ˜ íŒŒì„œ
def _as_bool(v: object | None, default: bool = False) -> bool:
    if isinstance(v, bool):
        return v
    if v is None:
        return default
    return str(v).strip().lower() in ("1", "true", "yes", "on")

def _as_int(v: object | None, default: int = 0) -> int:
    try:
        if v is None:
            return default
        s = str(v).strip()
        if s == "":
            return default
        return int(s)
    except Exception:
        return default


def _safe_json_block(s: str) -> Dict[str, Any]:
    s = (s or "").strip()
    i, j = s.find("{"), s.rfind("}")
    if i != -1 and j != -1 and j > i:
        s = s[i : j + 1]
    return json.loads(s)


def _listify(v: Any) -> List[str]:
    if v is None:
        return []
    if isinstance(v, list):
        return [str(x) for x in v if x is not None]
    return [str(v)]


def _dedupe_key(item: Dict[str, Any]) -> Dict[str, Any]:
    q: Dict[str, Any] = {}
    if item.get("article_code"):
        q["article_code"] = str(item["article_code"])
    elif item.get("article_id"):
        q["article_id"] = str(item["article_id"])
    elif item.get("url"):
        q["url"] = str(item["url"])
    return q


def _exists_in_news(q: Dict[str, Any]) -> bool:
    if not q:
        return False
    col = get_news_collection()
    if col is None:
        return False
    return col.find_one(q) is not None


# -------------------- ì´ˆê¸°í™” í•¨ìˆ˜(ìˆ˜ëª…ì£¼ê¸°ì—ì„œ í˜¸ì¶œ) --------------------

def init_index() -> dict:
    """ë‰´ìŠ¤ìš© Chroma ì»¬ë ‰ì…˜ ìƒì„±/ì—´ê¸°."""
    try:
        vs = create_chroma_store(
            collection_name=settings.news.collection,
            persist_dir=settings.news.persist_dir,
            distance="cosine",
        )
        vs.persist()
        return {
            "ok": True,
            "collection": settings.news.collection,
            "persist_dir": settings.news.persist_dir,
        }
    except Exception as e:
        return {"ok": False, "error": str(e)}


def init_news_index_semantic() -> dict:
    """
    ë‰´ìŠ¤ ì „ìš© 'ì‹œë§¨í‹± ì¸ë±ìŠ¤' ì´ˆê¸°í™”:
    - settings.news.api_urlì—ì„œ ê¸°ì‚¬ ë¡œë“œ â†’ ì‹œë§¨í‹± ì²­í‚¹ â†’ Chromaì— ì—…ì„œíŠ¸
    """
    try:
        if not settings.news.api_url:
            return {"ok": False, "reason": "settings.news.api_url is empty"}
        if HuggingFaceEmbeddings is None or SemanticChunker is None:
            return {"ok": False, "reason": "semantic stack unavailable (langchain_experimental / huggingface not installed)"}

        loader = NewsLoader(field_map=DEFAULT_FIELD_MAP)
        docs = loader.load_news_json(settings.news.api_url, timeout=15)
        if not docs:
            return {"ok": False, "reason": "no documents loaded", "api_url": settings.news.api_url}

        embed_for_chunk = HuggingFaceEmbeddings(model_name=settings.news.emb_model)
        splitter = SemanticChunker(embeddings=embed_for_chunk, breakpoint_threshold_type="percentile")
        chunks = splitter.split_documents(docs)

        vs = create_chroma_store(
            collection_name=settings.news.collection,
            persist_dir=settings.news.persist_dir,
            distance="cosine",
        )
        vs.vs.add_documents(chunks)
        vs.persist()

        return {
            "ok": True,
            "api_url": settings.news.api_url,
            "collection": settings.news.collection,
            "persist_dir": settings.news.persist_dir,
            "docs": len(docs),
            "chunks": len(chunks),
        }
    except Exception as e:
        return {"ok": False, "error": str(e)}


# -------------------- ë‹¨ê±´ ìƒì„± --------------------

def generate_news_post(
    req: NewsPublishRequest,
    feed_meta: Optional[Dict[str, Any]] = None,
    *,
    run_config: Optional[Dict[str, Any]] = None,   # â¬…ï¸ ì¶”ê°€: ìš”ì²­ ë‹¨ìœ„ íŠ¸ë ˆì´ìŠ¤ ì„¤ì •
) -> NewsPost:
    """
    .md í…œí”Œë¦¿ì„ ë¡œë“œí•˜ì—¬ LLMì— ì „ë‹¬.
    - retriever ì—†ìŒ(ë‰´ìŠ¤ìš© ë‹¨ê±´ ìš”ì•½/ê°€ê³µ)
    - ë¼ìš°í„°ì—ì„œ ë°›ì€ run_configë¥¼ ì²´ì¸ invokeì— ê·¸ëŒ€ë¡œ ì „ë‹¬
    """
    # 1) í…œí”Œë¦¿ ë¡œë”©/ë Œë”ë§
    tpl = load_md_template(settings.news.prompt_path or "src/prompts/templates/news_publish_v1.md")

    # ì•ˆì „ì¥ì¹˜: ì¸ì‚¬ì´íŠ¸ í…œí”Œë¦¿ ì„ì„ ì°¨ë‹¨
    _tpl_str = str(tpl)
    if ("Issue List" in _tpl_str) or ("target-insight" in _tpl_str):
        raise RuntimeError("ë‰´ìŠ¤ ì¶œê°„ì—ì„œ ì¸ì‚¬ì´íŠ¸ í…œí”Œë¦¿ì´ ê°ì§€ë¨. settings.news.prompt_pathë¥¼ í™•ì¸í•˜ì„¸ìš”.")

    prompt = render_template(
        tpl,
        title=(req.title or ""),
        url=(req.url or ""),
        content=(req.content or ""),
        categories=", ".join(req.categories or []),
        tags=", ".join(req.tags or []),
        meta=json.dumps(feed_meta or {}, ensure_ascii=False),
    )

    # 2) ìš”ì•½/ê°€ê³µ
    if not settings.news.use_llm:
        text = json.dumps({
            "title": req.title or "Untitled",
            "subtitle": None,
            "tldr": [],
            "body_md": req.content or "",
            "tags": req.tags or [],
            "sources": [req.url] if req.url else [],
            "hero_image_url": None,
            "author_name": getattr(req, "author_name", None),
        }, ensure_ascii=False)
    else:
        llm = build_default_llm()
        chain = build_news_chain(llm, settings.news.prompt_path or "src/prompts/templates/news_publish_v1.md")
        inputs = {
            "title": (req.title or ""),
            "url": (req.url or ""),
            "content": (req.content or ""),
            "categories": ", ".join(req.categories or []),
            "tags": ", ".join(req.tags or []),
            "meta": json.dumps(feed_meta or {}, ensure_ascii=False),
        }
        
        # generate_news_post ë‚´ë¶€, chain.invoke ì „ì— (ë””ë²„ê·¸ìš©)
        if run_config is None or not isinstance(run_config, dict) or not run_config.get("callbacks"):
            print("[warn] news run_config.callbacks missing; LangSmith project may fallback to env")
        else:
            # ì½œë°± í´ë˜ìŠ¤/í”„ë¡œì íŠ¸ëª… ëŒ€ëµ í™•ì¸
            try:
                cb = run_config["callbacks"][0]
                print("[diag] news callback tracer =", type(cb).__name__, getattr(cb, "project_name", None))
            except Exception:
                pass
        
        # â¬‡ï¸ ì¤‘ìš”: ìš”ì²­ ë‹¨ìœ„ LangSmith ì„¤ì • ì „ë‹¬
        text = chain.invoke(inputs, config=(run_config or {}))

    llm_out = _parse_llm_output(text)

    title = (llm_out.title if llm_out and llm_out.title else (req.title or "Untitled"))
    body_md = (llm_out.body_md if llm_out and llm_out.body_md else (req.content or ""))
    tldr_list = (llm_out.tldr if llm_out and llm_out.tldr else _fallback_tldr_from_text(body_md, k=3))
    tags = (llm_out.tags if llm_out and llm_out.tags else (req.tags or []))
    sources = (llm_out.sources if llm_out and llm_out.sources else [])
    hero = (llm_out.hero_image_url if llm_out and llm_out.hero_image_url else None)
    author = (llm_out.author_name if llm_out and llm_out.author_name else getattr(req, "author_name", None))
    category1 = (req.categories[0] if req.categories else None)

    post = NewsPost(
        post_id=str(uuid4()),
        article_id=req.article_id,
        article_code=getattr(req, "article_code", None),
        url=req.url,
        title=title,
        dek=(llm_out.subtitle if llm_out and llm_out.subtitle else None),
        tldr=tldr_list,
        body_md=body_md,
        hero_image_url=hero,
        author_name=author,
        category=category1,
        categories=req.categories or [],
        tags=tags,
        sources=sources,
        status="published" if req.publish else "draft",
        model_meta={
            "persona": "news_brief",
            "strategy": "auto",
            "top_k": req.top_k,
            "feed": feed_meta or {},
            "parsed_ok": bool(llm_out is not None),
            "service": getattr(settings.news, "service_name", "redfin_news"),
            "ls_project": getattr(settings.news, "langsmith_project", None),
        },
    )
    post.model_meta["urls"] = {"api": f"/redfin_news/posts/{post.post_id}"}

    col = get_news_collection()
    if col is not None:
        try:
            col.insert_one(post.dict())
        except Exception:
            pass

    # ì¶œê°„ ì§í›„ ìë™ ì¸ë±ì‹±
    try:
        if getattr(settings.news, "index_on_publish", False):
            upsert_news_post_to_chroma(post)
    except Exception:
        pass

    return post


# -------------------- NewsLoader ê¸°ë°˜ ë°°ì¹˜ ì¶œê°„ --------------------

def _req_from_loader_doc(doc) -> NewsPublishRequest:
    md = dict(doc.metadata or {})
    content = doc.page_content or ""
    categories: List[str] = []
    if md.get("content_type"):
        categories = [str(md["content_type"])]
    elif md.get("source"):
        categories = [str(md["source"])]
    return NewsPublishRequest(
        article_id=str(md.get("guid")) if md.get("guid") else None,
        article_code=str(md.get("guid") or md.get("doc_id")) if (md.get("guid") or md.get("doc_id")) else None,
        url=md.get("url"),
        title=md.get("title"),
        content=content,
        categories=categories,
        tags=[str(t) for t in (md.get("tags") or [])],
        publish=True,
        top_k=6,
    )


def publish_from_loader(
    feed_url: str,
    field_map: Optional[Dict[str, str]] = None,
    default_publish: bool = True,
    default_top_k: int = 6,
    timeout: int = 15,
    *,
    run_config: Optional[Dict[str, Any]] = None,   # â¬…ï¸ ì¶”ê°€
) -> Dict[str, Any]:
    loader = NewsLoader(field_map=field_map or DEFAULT_FIELD_MAP)
    docs = loader.load_news_json(feed_url, timeout=timeout)

    created, skipped, errors = [], [], []
    for idx, doc in enumerate(docs):
        try:
            md = dict(doc.metadata or {})
            req = _req_from_loader_doc(doc)

            # ê¸°ë³¸ê°’ ì ìš©
            if default_publish is not None:
                req.publish = bool(default_publish)
            if default_top_k is not None:
                try:
                    req.top_k = int(default_top_k)
                except Exception:
                    req.top_k = 6

            # ì¤‘ë³µ ì²´í¬
            q = _dedupe_key({
                "article_code": req.article_code,
                "article_id": req.article_id,
                "url": req.url,
            }) or ({"article_code": str(md.get("doc_id"))} if md.get("doc_id") else {})
            if q and _exists_in_news(q):
                skipped.append({"index": idx, "reason": "duplicate", "key": q})
                continue

            post = generate_news_post(req, feed_meta=md, run_config=run_config)  # â¬…ï¸ ì „ë‹¬
            created.append(post.dict())
        except Exception as e:
            errors.append({"index": idx, "error": str(e)})
    return {"created": created, "skipped": skipped, "errors": errors}


# -------------------- ê¸°ì¡´ JSON ë°°ì—´ ë°°ì¹˜(ì˜µì…˜ìœ¼ë¡œ ìœ ì§€) --------------------

def publish_batch(
    items: List[Dict[str, Any]],
    field_map: Optional[Dict[str, str]] = None,
    default_publish: bool = True,
    default_top_k: int = 6,
    *,
    run_config: Optional[Dict[str, Any]] = None,   # â¬…ï¸ ì¶”ê°€
) -> Dict[str, Any]:
    created, skipped, errors = [], [], []
    m = field_map or {}
    for idx, it in enumerate(items):
        try:
            article_code = str(it.get(m.get("article_code", "article_code")) or it.get("guid") or it.get("id") or "")
            article_id = str(it.get(m.get("article_id", "article_id")) or it.get("guid") or "")
            url = it.get(m.get("url", "url")) or it.get("link")
            title = it.get(m.get("title", "title"))
            content = it.get(m.get("content", "content")) or it.get("article_text")
            categories = it.get(m.get("categories", "categories")) or it.get("content_type")
            tags = it.get(m.get("tags", "tags")) or []

            req = NewsPublishRequest(
                article_id=article_id or None,
                article_code=article_code or None,
                url=url,
                title=title,
                content=content,
                categories=[categories] if isinstance(categories, str) else (categories or []),
                tags=[str(t) for t in (tags if isinstance(tags, list) else [tags] if tags else [])],
                publish=bool(it.get(m.get("publish", "publish"), default_publish)),
                top_k=int(it.get(m.get("top_k", "top_k"), default_top_k)),
            )

            q = _dedupe_key({"article_code": req.article_code, "article_id": req.article_id, "url": req.url})
            if q and _exists_in_news(q):
                skipped.append({"index": idx, "reason": "duplicate", "key": q})
                continue

            post = generate_news_post(req, feed_meta=it, run_config=run_config)  # â¬…ï¸ ì „ë‹¬
            created.append(post.dict())
        except Exception as e:
            errors.append({"index": idx, "error": str(e)})
    return {"created": created, "skipped": skipped, "errors": errors}


# -------------------- ì™¸ë¶€ í˜¸ì¶œ ì§„ì…ì  --------------------

def publish_from_feed(
    feed_url: str,
    item_path: Optional[str] = None,      # ë³´ì¡´(ë¯¸ì‚¬ìš©)
    field_map: Optional[Dict[str, str]] = None,
    default_publish: bool = True,
    default_top_k: int = 6,
    *,
    run_config: Optional[Dict[str, Any]] = None,   # â¬…ï¸ ì¶”ê°€
) -> Dict[str, Any]:
    return publish_from_loader(
        feed_url=feed_url,
        field_map=field_map,
        default_publish=default_publish,
        default_top_k=default_top_k,
        timeout=15,
        run_config=run_config,             # â¬…ï¸ ì „ë‹¬
    )


def publish_from_env(*, run_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:  # â¬…ï¸ ì‹œê·¸ë‹ˆì²˜ ë³€ê²½
    # 1) API URL
    feed_url = settings.news.api_url or os.getenv("NEWS_API_URL")
    if not feed_url:
        raise ValueError("NEWS_API_URL/settings.news.api_url is empty")

    # 2) í•„ë“œ ë§¤í•‘
    user_map = getattr(settings.news, "feed_field_map", None)
    if user_map is None:
        field_map_json = os.getenv("NEWS_FEED_FIELD_MAP")
        user_map = json.loads(field_map_json) if field_map_json else None
    field_map = {**DEFAULT_FIELD_MAP, **user_map} if user_map else DEFAULT_FIELD_MAP

    # 3) í¼ë¸”ë¦¬ì‹œ/íƒ‘K
    default_publish = _as_bool(os.getenv("NEWS_DEFAULT_PUBLISH", None),
                               default=_as_bool(getattr(settings.news, "default_publish", True), True))
    default_top_k = _as_int(os.getenv("NEWS_TOP_K", None),
                            default=_as_int(getattr(settings.news, "top_k", 6), 6))

    # 4) ì‹¤í–‰
    return publish_from_loader(
        feed_url=feed_url,
        field_map=field_map,
        default_publish=default_publish,
        default_top_k=default_top_k,
        timeout=15,
        run_config=run_config,             # â¬…ï¸ ì „ë‹¬
    )


# ------------------- í—¬í¼ -------------------

def _fallback_tldr_from_text(text: str, k: int = 3) -> List[str]:
    sents = re.split(r"(?<=[.!?ã€‚ï¼ï¼Ÿ])\s+", (text or "").strip())
    out = []
    for s in sents:
        s = s.strip()
        if not s:
            continue
        out.append(s[:90])
        if len(out) >= max(1, k):
            break
    return out

def _parse_llm_output(text: str) -> Optional[NewsLLMOut]:
    try:
        data = _safe_json_block(text)
    except Exception:
        return None
    try:
        if hasattr(NewsLLMOut, "model_validate"):
            return NewsLLMOut.model_validate(data)   # pydantic v2
        return NewsLLMOut.parse_obj(data)            # pydantic v1
    except Exception:
        return None


# ------------------- ê³ ì •í¬ê¸° ì²­í‚¹ + ì˜¤ë²„ë© ì¸ë±ìŠ¤ ì´ˆê¸°í™” -------------------

def init_news_index_fixed(
    *,
    chunk_size: int = 1200,
    chunk_overlap: int = 120,
) -> dict:
    try:
        if not settings.news.api_url:
            return {"ok": False, "reason": "settings.news.api_url is empty"}

        loader = NewsLoader(field_map=DEFAULT_FIELD_MAP)
        docs = loader.load_news_json(settings.news.api_url, timeout=15)
        if not docs:
            return {"ok": False, "reason": "no documents loaded", "api_url": settings.news.api_url}

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", "? ", "! ", ", ", " ", ""],
        )
        chunks = splitter.split_documents(docs)

        embedding = HuggingFaceEmbeddings(model_name=settings.news.emb_model)

        vs = create_chroma_store(
            embedding=embedding,
            collection_name=settings.news.collection,
            persist_dir=settings.news.persist_dir,
            distance="cosine",
        )
        vs.vs.add_documents(chunks)
        vs.persist()

        return {
            "ok": True,
            "mode": "fixed",
            "api_url": settings.news.api_url,
            "collection": settings.news.collection,
            "persist_dir": settings.news.persist_dir,
            "docs": len(docs),
            "chunks": len(chunks),
            "chunk_size": chunk_size,
            "chunk_overlap": chunk_overlap,
        }
    except Exception as e:
        return {"ok": False, "error": str(e)}

def _get_llm_for_news():
    """ì¶œê°„ìš© LLM. NEWS__USE_LLM=falseë©´ None ë°˜í™˜í•´ì„œ íŒ¨ìŠ¤ìŠ¤ë£¨ ì €ì¥."""
    from core import settings
    if not getattr(settings.news, "use_llm", True):
        return None
    # ìš°ì„ ìˆœìœ„: GOOGLE â†’ OPENAI
    import os
    if os.getenv("GOOGLE_API_KEY"):
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model=os.getenv("GEMINI_MODEL","gemini-2.0-flash"), temperature=0)
    if os.getenv("OPENAI_API_KEY"):
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model=os.getenv("MODEL_LLM","gpt-4o-mini"), temperature=0)
    # í‚¤ê°€ ì—†ìœ¼ë©´ íŒ¨ìŠ¤ìŠ¤ë£¨
    return None

def _render_body_md(raw_item: Dict[str, Any], llm, prompt_path: str) -> str:
    """í…œí”Œë¦¿+LLMë¡œ ìš”ì•½ ë³¸ë¬¸ ìƒì„±. LLMì´ ì—†ìœ¼ë©´ íŒ¨ìŠ¤ìŠ¤ë£¨(ê¸°ì‚¬ ë³¸ë¬¸ ì¼ë¶€/ì œëª©ë§Œ)."""
    title = raw_item.get("title") or ""
    content = raw_item.get("article_text") or raw_item.get("content") or ""
    if not llm:
        # íŒ¨ìŠ¤ìŠ¤ë£¨: ì œëª© + ë³¸ë¬¸ ì•ë¶€ë¶„ 800ì
        return f"# {title}\n\n{content[:800]}"

    # í…œí”Œë¦¿ ë¡œë”©
    from pathlib import Path
    tpl = ""
    try:
        p = Path(prompt_path)
        tpl = p.read_text(encoding="utf-8") if p.exists() else ""
    except Exception:
        tpl = ""

    # ê°„ë‹¨ í”„ë¡¬í”„íŠ¸
    system = "ë‹¹ì‹ ì€ ê¸°ìì…ë‹ˆë‹¤. í•µì‹¬ë§Œ ê°„ê²°íˆ ìš”ì•½í•˜ê³ , ì†Œì œëª©ê³¼ ë¶ˆë¦¿ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
    user = f"{tpl}\n\n[ì œëª©]\n{title}\n\n[ë³¸ë¬¸]\n{content}\n\nìœ„ ë‚´ìš©ì„ ê°„ê²°í•œ ê¸°ì‚¬ ìš”ì•½(Markdown)ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."
    try:
        resp = llm.invoke([("system", system), ("user", user)])
        text = getattr(resp, "content", None) or str(resp)
        return text.strip()
    except Exception as e:
        return f"# {title}\n\n{content[:800]}\n\n<!-- LLM ì‹¤íŒ¨: {e} -->"

def publish_from_source(*, mongo_query: Optional[Dict[str, Any]] = None, limit: int = 30) -> Dict[str, Any]:
    """
    Mongo redfin.extract ì—ì„œ ê¸°ì‚¬ ë¡œë“œ â†’ (ì„ íƒ)ìš”ì•½ â†’ redfin.news_logsì— upsert ì €ì¥.
    - retriever ì‚¬ìš© ì—†ìŒ
    - api_url ì˜ì¡´ ì—†ìŒ
    """
    from core import settings
    from nureongi.loaders import NewsLoader

    # 1) ì…ë ¥ ë¡œë“œ (Mongo ìµœì‹  ìš°ì„ , ìµœëŒ€ limit)
    loader = NewsLoader()
    docs = loader.load_news_mongo(
        uri=settings.mongo.uri,
        db=settings.mongo.db,
        collection=getattr(settings.news, "source_collection", "extract"),
        query=mongo_query or {},
        limit=limit,
        timeout_ms=getattr(settings.mongo, "timeout_ms", 3000),
        sort=[("processed_at", -1), ("published", -1)],
    )
    if not docs:
        return {"ok": False, "reason": "no input docs from mongo.extract"}

    # 2) ìš”ì•½/ë³¸ë¬¸ ìƒì„±
    llm = _get_llm_for_news()
    posts: List[Dict[str, Any]] = []
    for d in docs:
        md = d.metadata or {}
        raw = {
            "title": md.get("title"),
            "article_text": d.page_content,  # loadersì—ì„œ title+contentë¡œ page_content êµ¬ì„±í•œ ê²½ìš°, ìš”ì•½ í•¨ìˆ˜ì—ì„œ ë‹¤ì‹œ ì˜ë¼ ì”€
            "source": md.get("source"),
            "link": md.get("url") or md.get("link"),
            "guid": md.get("guid"),
            "published_at": md.get("published_at"),
            "tags": md.get("tags") or [],
            "authors": md.get("authors") or [],
            "language": md.get("lang"),
        }
        body_md = _render_body_md(raw, llm, prompt_path=getattr(settings.news, "prompt_path", "src/prompts/templates/news_publish_v1.md"))
        posts.append({
            "type": "post",
            "title": raw["title"],
            "body_md": body_md,
            "source": raw["source"],
            "link": raw["link"],
            "guid": raw["guid"],
            "published_at": raw["published_at"],
            "tags": raw["tags"],
            "authors": raw["authors"],
            "lang": raw["language"],
            "created_at": datetime.utcnow(),
            "updated_at": datetime.utcnow(),
        })

    # 3) Mongo ì €ì¥ (upsert by guid/link)
    client = MongoClient(settings.mongo.uri, serverSelectionTimeoutMS=settings.mongo.timeout_ms)
    coll = client[settings.mongo.db][getattr(settings.mongo, "news_collection", "news_logs")]

    bulk: List[UpdateOne] = []
    for p in posts:
        filt = {"$or": [{"guid": p.get("guid")}, {"link": p.get("link")}]}
        bulk.append(UpdateOne(filt, {"$set": p}, upsert=True))

    if bulk:
        res = coll.bulk_write(bulk, ordered=False)
        upserts = len(res.upserted_ids or {})
        modified = res.modified_count
    else:
        upserts = modified = 0

    return {"ok": True, "count": len(posts), "upserts": upserts, "modified": modified}
